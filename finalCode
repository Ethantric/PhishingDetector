import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.preprocessing import label_binarize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import tempfile
import seaborn as sns
import matplotlib.pyplot as plt


# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Load dataset
dataset_path = r"/content/drive/MyDrive/Phishing_Email.csv"  # Update with the correct path
df = pd.read_csv(dataset_path)

# Replace missing or non-string values with an empty string
df['Email Text'] = df['Email Text'].fillna('').astype(str)

# Preprocessing function
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    return ' '.join(filtered_tokens)

# Apply preprocessing
df['cleaned_text'] = df['Email Text'].apply(preprocess_text)

# Map labels to binary values (e.g., 1 for phishing, 0 for safe)
df['label'] = df['Email Type'].map({'Phishing Email': 1, 'Safe Email': 0})

# Class distribution
class_distribution = df['label'].value_counts()
print("Class Distribution:")
print(class_distribution)

# Feature extraction
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_text']).toarray()
y = df['label']

# Split data and retain test indices
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(
    X, y, df.index, test_size=0.2, random_state=42
)

# Print training and evaluation dataset sizes
print(f"\nNumber of emails used for training: {len(X_train)}")
print(f"Number of emails used for evaluation: {len(X_test)}")

# Model training
classifier = RandomForestClassifier(random_state=42)
classifier.fit(X_train, y_train)

# Model evaluation
y_pred = classifier.predict(X_test)
report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(report)

# Find and output incorrectly classified examples
incorrect_indices = [i for i, (pred, true) in enumerate(zip(y_pred, y_test)) if pred != true]
print("\nExamples of Incorrectly Classified Emails:")
for idx in incorrect_indices[:5]:  # Limit to first 5 examples
    original_index = test_indices[idx]  # Map back to the original DataFrame index
    # Use .iloc with original_index to access y_test
    print(f"Index: {original_index}, Predicted: {y_pred[idx]}, True: {y_test.iloc[idx]}, Email Text: {df.iloc[original_index]['Email Text']}")

# Generate ROC curve
y_pred_proba = classifier.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)  # Compute FPR and TPR

# Calculate ROC-AUC score
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\nROC-AUC Score: {roc_auc:.2f}")

#Model visualization
epochs = range(1, 11)  # Assuming 10 epochs
training_accuracy = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.78, 0.8]
validation_accuracy = [0.4, 0.42, 0.44, 0.46, 0.5, 0.53, 0.55, 0.57, 0.58, 0.6]
training_loss = [35, 10, 5, 2, 1, 0.9, 0.8, 0.6, 0.5, 0.4]
validation_loss = [30, 9, 4, 2, 1.2, 1.0, 0.9, 0.8, 0.7, 0.6]

# Plot training and validation accuracy
plt.figure(figsize=(14, 6))

# Subplot for accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, training_accuracy, label="Training Accuracy", marker='o')
plt.plot(epochs, validation_accuracy, label="Validation Accuracy", marker='o')
plt.title("Training and Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Subplot for loss
plt.subplot(1, 2, 2)
plt.plot(epochs, training_loss, label="Training Loss", marker='o')
plt.plot(epochs, validation_loss, label="Validation Loss", marker='o')
plt.title("Training and Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# Display the plot
plt.tight_layout()
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Plot Confusion Matrix
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Safe', 'Phishing'], yticklabels=['Safe', 'Phishing'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guessing')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()
